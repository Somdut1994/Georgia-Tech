{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Avondale-HW1.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"TfXXhr8c_G7m","colab_type":"code","colab":{}},"source":["import pandas as pd\n","import numpy as np\n","import scipy as sp\n","import re\n","import nltk\n","from bs4 import BeautifulSoup\n","from nltk.corpus import stopwords\n","from copy import deepcopy\n","from sklearn.feature_extraction.text import CountVectorizer\n","from sklearn.feature_extraction.text import TfidfTransformer\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.metrics import accuracy_score, f1_score\n","import random as rd\n","from sklearn.model_selection import train_test_split, KFold\n","from sklearn.neighbors import NearestNeighbors\n","nltk.download('stopwords')"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"kdG0cKu8Btxw","colab_type":"text"},"source":["Importing data..."]},{"cell_type":"code","metadata":{"id":"RxihJYe6CMd1","colab_type":"code","outputId":"b6a4e6a9-6edc-47b9-e0df-5d7e584dff4f","executionInfo":{"status":"ok","timestamp":1583469838146,"user_tz":300,"elapsed":32466,"user":{"displayName":"Devanshee Shah","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiPK61duZO3eD2_k6CihN89Q3LYOuSUxY6MRWaz4Q=s64","userId":"17675293810520504748"}},"colab":{"base_uri":"https://localhost:8080/","height":122}},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n","\n","Enter your authorization code:\n","··········\n","Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"ZEolkFp7ACue","colab_type":"code","colab":{}},"source":["data = pd.read_csv(\"/content/drive/My Drive/Avondale_Restaurant_Review.csv\", delimiter=\",\", engine='python')"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"tlLb_16Vfmd_","colab_type":"code","colab":{}},"source":["def preprocess_review(review):\n","    \"\"\"Helper function to clean the reviews.\n","\n","     Arg: review: review text.\n","     Returns: clean_review : Cleaned reviews\n","\n","     You should carry out the following steps.\n","     1. Remove HTML Tags.\n","     2. Remove non-letter characters.\n","     3. Convert to lower case.\n","     4. Remove stopwords.\n","    \"\"\"\n","\n","    #Write your code below.\n","    \n","    ## HTML Tags removal\n","    clean1=BeautifulSoup(review)  \n","    \n","    ## Non-letter characters removal\n","    clean2=re.sub(\"[^a-zA-Z]\", \" \", clean1.get_text()) \n","    \n","    ## Converting to lower case and splitting each word\n","    clean3=clean2.lower().split()\n","    \n","    ## Removing stopwords\n","    stops = set(stopwords.words(\"english\"))\n","    clean4=[w for w in clean3 if not w in stops]\n","    \n","    ## Returning the reviews with spaces between words\n","    clean_review=\" \".join(clean4)\n","\n","    \n","    return clean_review"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"AzhnmllbByg3","colab_type":"text"},"source":["Splitting >=4 and 4 out of 5 as good (1) and NOT good (0)."]},{"cell_type":"code","metadata":{"id":"bgHFPtYcg3R5","colab_type":"code","outputId":"3866402c-50c9-4ad6-dad2-3957669237ab","executionInfo":{"status":"ok","timestamp":1583469888650,"user_tz":300,"elapsed":1381,"user":{"displayName":"Devanshee Shah","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiPK61duZO3eD2_k6CihN89Q3LYOuSUxY6MRWaz4Q=s64","userId":"17675293810520504748"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["data.loc[data['stars'] > 4, 'Good Review'] = 1\n","data.loc[data['stars'] <= 4, 'Good Review'] = 0\n","print(sum(data['Good Review'] == 0) / len(data['Good Review']) * 100, 'percent of reviews are bad (less than 5 star).')"],"execution_count":0,"outputs":[{"output_type":"stream","text":["58.600537039962084 percent of reviews are bad (less than 5 star).\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"2J6LgZ1shjkT","colab_type":"code","colab":{}},"source":["def preprocess_review(review):\n","    \"\"\"Helper function to clean the reviews.\n","\n","     Arg: review: review text.\n","     Returns: clean_review : Cleaned reviews\n","\n","     You should carry out the following steps.\n","     1. Remove HTML Tags.\n","     2. Remove non-letter characters.\n","     3. Convert to lower case.\n","     4. Remove stopwords.\n","    \"\"\"\n","\n","    #Write your code below.\n","    \n","    ## HTML Tags removal\n","    clean1=BeautifulSoup(review)  \n","    \n","    ## Non-letter characters removal\n","    clean2=re.sub(\"[^a-zA-Z]\", \" \", clean1.get_text()) \n","    \n","    ## Converting to lower case and splitting each word\n","    clean3=clean2.lower().split()\n","    \n","    ## Removing stopwords\n","    stops = set(stopwords.words(\"english\"))\n","    clean4=[w for w in clean3 if not w in stops]\n","    \n","    ## Returning the reviews with spaces between words\n","    clean_review=\" \".join(clean4)\n","\n","    \n","    return clean_review"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"TFlv0Tt3B9CF","colab_type":"text"},"source":["80-20 split on the data and then cleaning the reviews based on method described in HW1."]},{"cell_type":"code","metadata":{"id":"RTEu81FQpydC","colab_type":"code","outputId":"43015156-b8ea-4017-eb37-703948f3e76f","executionInfo":{"status":"ok","timestamp":1583469922968,"user_tz":300,"elapsed":6980,"user":{"displayName":"Devanshee Shah","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiPK61duZO3eD2_k6CihN89Q3LYOuSUxY6MRWaz4Q=s64","userId":"17675293810520504748"}},"colab":{"base_uri":"https://localhost:8080/","height":71}},"source":["msk = np.random.rand(len(data)) < 0.8\n","train = data[msk]\n","test = data[~msk]\n","\n","#Clean the reviews and add them to the list below\n","reviews_train_clean = []\n","reviews_test_clean = []\n","#Write your code below.\n","num_reviews = train[\"text\"].size\n","tlist=train[\"text\"].tolist()\n","for i in range(num_reviews):\n","  cr=preprocess_review(tlist[i])\n","  reviews_train_clean.append(cr)\n","\n","num_reviews = test[\"text\"].size\n","tlist=test[\"text\"].tolist()\n","for i in range(num_reviews):\n","    cr=preprocess_review(tlist[i])\n","    reviews_test_clean.append(cr)    "],"execution_count":0,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/bs4/__init__.py:273: UserWarning: \"b'..'\" looks like a filename, not markup. You should probably open this file and pass the filehandle into Beautiful Soup.\n","  ' Beautiful Soup.' % markup)\n"],"name":"stderr"}]},{"cell_type":"markdown","metadata":{"id":"FlVHgPcACJEW","colab_type":"text"},"source":["Formed design matrices: Conts, Binary and TFIDF..."]},{"cell_type":"code","metadata":{"id":"9AcWKpo2qjTW","colab_type":"code","colab":{}},"source":["def design_matrix(cleaned_reviews, cv):\n","\n","    data_features = cv.transform(cleaned_reviews)\n","\n","    X_counts = data_features.toarray()\n","\n","    X_binary=X_counts\n","\n","    X_binary[X_binary>0]=1\n","\n","    transformer = TfidfTransformer()\n","\n","    X_tfidf = transformer.fit_transform(X_counts).toarray()\n","   \n","    return X_counts,X_binary,X_tfidf"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"uWMYqbbLq4Ld","colab_type":"code","colab":{}},"source":["cv = CountVectorizer(analyzer = \"word\",   \\\n","                            tokenizer = None,    \\\n","                            preprocessor = None, \\\n","                            stop_words = None,   \\\n","                            max_features = 500) \n","\n","cv.fit(reviews_train_clean)\n","\n","X_train_counts,X_train_binary,X_train_tfidf = design_matrix(reviews_train_clean, cv) \n","X_test_counts,X_test_binary,X_test_tfidf = design_matrix(reviews_test_clean, cv)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"cxHn4foIkbLQ","colab_type":"code","colab":{}},"source":["from warnings import filterwarnings\n","filterwarnings('ignore')\n","import time\n","\n","def calculateF1(X, y, k = 5):\n","    \"\"\"calculateF1(X, y, k = 5) return two list which record all randomly selected c(in the interval (1e-4, 1e4))\n","     and corresponding F1 scores.\n","\n","     Args: X: Features\n","           y: Label of sentiment\n","           k: Number of Cross-validation\n","\n","     Returns: c_list: List of all c values.\n","              f1_list: Corresponding F1 Scores.\n","    \"\"\"\n","    rd.seed(0) #Setting a common seed\n","\n","    #Write your code here.\n","    start=time.time()\n","    c_list=[10**i for i in np.random.uniform(-4, 4, 30)]\n","    f1_list=[]\n","    kf = KFold(n_splits=k)\n","    print('itr# Time')\n","    i=0\n","    for c in c_list:\n","        f1score=0.0\n","        i+=1\n","        for train_index, test_index in kf.split(X):\n","            X_train, X_test = X[train_index], X[test_index]\n","            y_train, y_test = y[train_index], y[test_index]\n","            clf = LogisticRegression(C=c)\n","            clf = clf.fit(X_train, y_train)\n","            y_pred_test = clf.predict(X_test)\n","            f1score+=f1_score(y_test, y_pred_test)/float(k)\n","        f1_list.append(f1score)\n","        if i%5==0:\n","            print(i, time.time()-start)        \n","        \n","    return c_list, f1_list"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"tFFX2j-hkjGP","colab_type":"code","colab":{}},"source":["def findBestC(X, y, k = 5):\n","    \"\"\"findBestC(X, y, k) return the best performance c, and the improvement(difference between best and worst f1_scores)/\n","     Args: X: Features\n","           y: Label of sentiment\n","           k: Number of Cross-validation\n","     Returns: c_best: C value with best f1_score.\n","              improvement: difference between best and worst f1_score.\n","    \"\"\"\n","    #Write your code here. \n","    c_list, f1_list = calculateF1(X, y, k)\n","    f1max = max(f1_list)\n","    c_best = c_list[f1_list.index(f1max)]\n","    improvement = f1max - min(f1_list)\n","    return c_best,improvement"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"xkqlM9uwkvMQ","colab_type":"code","colab":{}},"source":["def findImprovement(X,train_sentiment,test_size = 0.2, random_state = 0):\n","    \"\"\" Find the improvement in F1-Score of the design Matrix(X) using previous utility functions and the test_f1_score using the best C.\n","\n","      Args: X: Design Matrix\n","            train_sentiment: Sentiments of the training data\n","            test_size: Split it as 80:20\n","            random_state: Seed\n","\n","      Returns:\n","            c_best: The best possible c value\n","            improvement: improvement in F1-Score using the design Matrix(X).\n","            f1_s: Test F1 Score.\n","            \n","\n","      You should carry out the following Steps:\n","      1. Split the data using the above parameters.\n","      2. Find out the best c and the improvement. (use 5-fold Cross Validation.)\n","      3. Find out the test f1 score with this c.\n","    \"\"\"\n","    #Write your code here.\n","    X_train, X_test, y_train, y_test = train_test_split(X, train_sentiment, test_size=test_size, random_state=random_state)\n","    c_best, improvement = findBestC(X_train, y_train)\n","    clf = LogisticRegression(C=c_best)\n","    clf = clf.fit(X_train, y_train)\n","    y_pred_test = clf.predict(X_test)\n","    f1_s = f1_score(y_test, y_pred_test)   \n","\n","    return c_best,improvement,f1_s"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"TMlKsoZrtQeN","colab_type":"code","outputId":"a0a47a84-684a-4252-d238-ec40ca746067","executionInfo":{"status":"ok","timestamp":1583470089580,"user_tz":300,"elapsed":29828,"user":{"displayName":"Devanshee Shah","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiPK61duZO3eD2_k6CihN89Q3LYOuSUxY6MRWaz4Q=s64","userId":"17675293810520504748"}},"colab":{"base_uri":"https://localhost:8080/","height":153}},"source":["target_train = train['Good Review'].values\n","target_test = test['Good Review'].values\n","c_best,improvement,f1_s = findImprovement(X_train_counts, target_train)\n","print('best c:', c_best, '; improvement:', improvement, '; test f1_score:', f1_s)\n"],"execution_count":0,"outputs":[{"output_type":"stream","text":["itr# Time\n","5 4.9772186279296875\n","10 10.12351393699646\n","15 14.22546672821045\n","20 18.082712650299072\n","25 22.64530301094055\n","30 28.434545755386353\n","best c: 0.26647371234970335 ; improvement: 0.7538168063838642 ; test f1_score: 0.7448609431680774\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"PNV0dg7d2nR3","colab_type":"code","outputId":"7be04952-1353-4da1-ede3-9a0846630951","executionInfo":{"status":"ok","timestamp":1583470097222,"user_tz":300,"elapsed":1044,"user":{"displayName":"Devanshee Shah","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiPK61duZO3eD2_k6CihN89Q3LYOuSUxY6MRWaz4Q=s64","userId":"17675293810520504748"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["clf = LogisticRegression(C=c_best)\n","clf = clf.fit(X_train_counts, target_train)\n","target_test_pred_counts = clf.predict(X_test_counts)\n","print('Counts Accuracy:', f1_score(target_test, target_test_pred_counts))"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Counts Accuracy: 0.7467312348668281\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"9jvcNfP6R4Tc","colab_type":"text"},"source":["# ***Using KNN Classifier ***"]},{"cell_type":"code","metadata":{"id":"-yn2-7FNRbmG","colab_type":"code","colab":{}},"source":["from warnings import filterwarnings\n","from sklearn.neighbors import KNeighborsClassifier\n","filterwarnings('ignore')\n","import time\n","\n","def calculateF1(X, y, k = 2):\n","    \"\"\"calculateF1(X, y, k = 5) return two list which record all randomly selected c(in the interval (1e-4, 1e4))\n","     and corresponding F1 scores.\n","\n","     Args: X: Features\n","           y: Label of sentiment\n","           k: Number of Cross-validation\n","\n","     Returns: c_list: List of all c values.\n","              f1_list: Corresponding F1 Scores.\n","    \"\"\"\n","    rd.seed(0) #Setting a common seed\n","\n","    #Write your code here.\n","    start=time.time()\n","    #c_list=[10**i for i in np.random.uniform(-4, 4, 30)]\n","    #c_list=[10,20,30,40,50,70,90,100,200]\n","    c_list=[10,20,40,70,100,200]\n","    f1_list=[]\n","    kf = KFold(n_splits=k)\n","    print('itr# Time')\n","    i=0\n","    for c in c_list:\n","        f1score=0.0\n","        i+=1\n","        for train_index, test_index in kf.split(X):\n","            X_train, X_test = X[train_index], X[test_index]\n","            y_train, y_test = y[train_index], y[test_index]\n","            # model = KNeighborsClassifier(n_neighbors=3)\n","            # model.fit(X_train,y_train)\n","            # y_pred_test =model.predict(X_test)\n","            # f1_s = f1_score(y_test, y_pred_test)   \n","            clf=KNeighborsClassifier(n_neighbors=c)\n","            clf = clf.fit(X_train, y_train)\n","            y_pred_test = clf.predict(X_test)\n","            f1score+=f1_score(y_test, y_pred_test)/float(k)\n","        f1_list.append(f1score)\n","        if i%1==0:\n","            print(i, time.time()-start)        \n","        \n","    return c_list, f1_list"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"mcw-JAz-RfPX","colab_type":"code","colab":{}},"source":["def findBestC(X, y, k = 2):\n","    \"\"\"findBestC(X, y, k) return the best performance c, and the improvement(difference between best and worst f1_scores)/\n","     Args: X: Features\n","           y: Label of sentiment\n","           k: Number of Cross-validation\n","     Returns: c_best: C value with best f1_score.\n","              improvement: difference between best and worst f1_score.\n","    \"\"\"\n","    #Write your code here. \n","    c_list, f1_list = calculateF1(X, y, k)\n","    f1max = max(f1_list)\n","    c_best = c_list[f1_list.index(f1max)]\n","    improvement = f1max - min(f1_list)\n","    return c_best,improvement"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"DFzhwXImRkKG","colab_type":"code","colab":{}},"source":["def findImprovement(X,train_sentiment,test_size = 0.2, random_state = 0):\n","    \"\"\" Find the improvement in F1-Score of the design Matrix(X) using previous utility functions and the test_f1_score using the best C.\n","\n","      Args: X: Design Matrix\n","            train_sentiment: Sentiments of the training data\n","            test_size: Split it as 80:20\n","            random_state: Seed\n","\n","      Returns:\n","            c_best: The best possible c value\n","            improvement: improvement in F1-Score using the design Matrix(X).\n","            f1_s: Test F1 Score.\n","            \n","\n","      You should carry out the following Steps:\n","      1. Split the data using the above parameters.\n","      2. Find out the best c and the improvement. (use 5-fold Cross Validation.)\n","      3. Find out the test f1 score with this c.\n","    \"\"\"\n","    #Write your code here.\n","    X_train, X_test, y_train, y_test = train_test_split(X, train_sentiment, test_size=test_size, random_state=random_state)\n","    c_best, improvement = findBestC(X_train, y_train)\n","    clf=KNeighborsClassifier(n_neighbors=c_best)\n","    clf = clf.fit(X_train, y_train)\n","    y_pred_test = clf.predict(X_test)\n","    f1_s = f1_score(y_test, y_pred_test)   \n","\n","    return c_best,improvement,f1_s"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"D81oOBHDcdrh","colab_type":"code","outputId":"3d1f25f4-252c-4d62-ccb3-c7b4b6ebb695","executionInfo":{"status":"ok","timestamp":1583443636186,"user_tz":300,"elapsed":2427255,"user":{"displayName":"Devanshee Shah","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiPK61duZO3eD2_k6CihN89Q3LYOuSUxY6MRWaz4Q=s64","userId":"17675293810520504748"}},"colab":{"base_uri":"https://localhost:8080/","height":153}},"source":["target_train = train['Good Review'].values\n","target_test = test['Good Review'].values\n","c_best,improvement,f1_s = findImprovement(X_train_counts, target_train)\n","print('best c:', c_best, '; improvement:', improvement, '; test f1_score:', f1_s)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["itr# Time\n","1 369.9298598766327\n","2 738.7925205230713\n","3 1109.9048051834106\n","4 1484.1028571128845\n","5 1861.7807123661041\n","6 2235.4009606838226\n","best c: 40 ; improvement: 0.06075199138192122 ; test f1_score: 0.6997929606625259\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"nzvBSjpFRxmU","colab_type":"code","outputId":"e916e170-16c9-448c-f4e2-962cc128814b","executionInfo":{"status":"ok","timestamp":1583444088451,"user_tz":300,"elapsed":274530,"user":{"displayName":"Devanshee Shah","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiPK61duZO3eD2_k6CihN89Q3LYOuSUxY6MRWaz4Q=s64","userId":"17675293810520504748"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["clf=KNeighborsClassifier(n_neighbors=c_best)\n","clf = clf.fit(X_train_counts, target_train)\n","target_test_pred_counts = clf.predict(X_test_counts)\n","print('Counts Accuracy:', f1_score(target_test, target_test_pred_counts))"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Counts Accuracy: 0.6887206058190514\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"PM0_wTdAmgGh","colab_type":"text"},"source":["# ***KNN finished***"]},{"cell_type":"code","metadata":{"id":"tqmNfYZN9pFx","colab_type":"code","outputId":"a4a3ba3a-24c0-4f77-a794-67deb8b84017","executionInfo":{"status":"ok","timestamp":1583366958198,"user_tz":300,"elapsed":356,"user":{"displayName":"Somdut Roy","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjIPvTBNREX_XzQ_ZDqoXvFJo8YhuXBgCcbEQt3jg=s64","userId":"04921546445191518648"}},"colab":{"base_uri":"https://localhost:8080/","height":272}},"source":["feature_to_coef = {\n","    word: coef for word, coef in zip(\n","        cv.get_feature_names(), clf.coef_[0]\n","    )\n","}\n","print('Good words:', end = \"\\n\\n\")\n","for best_positive in sorted(\n","    feature_to_coef.items(), \n","    key=lambda x: x[1], \n","    reverse=True)[:5]:\n","    print (best_positive)\n","print('')\n","print('Bad words:', end = \"\\n\\n\")\n","for best_negative in sorted(\n","    feature_to_coef.items(), \n","    key=lambda x: x[1])[:5]:\n","    print (best_negative)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Good words:\n","\n","('amazing', 1.1231362991731006)\n","('delicious', 0.8682917821078282)\n","('thank', 0.7455727848180447)\n","('incredible', 0.7081842348265955)\n","('best', 0.7024155073245454)\n","\n","Bad words:\n","\n","('worst', -1.1174352432449874)\n","('ok', -0.9557728026765702)\n","('rude', -0.9388451010331978)\n","('horrible', -0.9195265109504851)\n","('bland', -0.8674215575962614)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"nmjDluEECoRX","colab_type":"code","outputId":"ef4eb3ee-ae33-4088-83f1-9d9eacc71c34","executionInfo":{"status":"ok","timestamp":1583367706998,"user_tz":300,"elapsed":446270,"user":{"displayName":"Somdut Roy","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjIPvTBNREX_XzQ_ZDqoXvFJo8YhuXBgCcbEQt3jg=s64","userId":"04921546445191518648"}},"colab":{"base_uri":"https://localhost:8080/","height":153}},"source":["c_best,improvement,f1_s = findImprovement(X_train_tfidf, target_train)\n","print('best c:', c_best, '; improvement:', improvement, '; test f1_score:', f1_s)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["itr# Time\n","5 58.34920382499695\n","10 119.7146646976471\n","15 168.73808670043945\n","20 269.6752185821533\n","25 337.96801376342773\n","30 444.0407249927521\n","best c: 1.5433124001908993 ; improvement: 0.7556461423661043 ; test f1_score: 0.7592592592592593\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"znOlU9onCybw","colab_type":"code","outputId":"a64cd416-7ee5-4dfb-9869-552c34bcef19","executionInfo":{"status":"ok","timestamp":1583367748737,"user_tz":300,"elapsed":2584,"user":{"displayName":"Somdut Roy","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjIPvTBNREX_XzQ_ZDqoXvFJo8YhuXBgCcbEQt3jg=s64","userId":"04921546445191518648"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["clf = LogisticRegression(C=c_best)\n","clf = clf.fit(X_train_tfidf, target_train)\n","target_test_pred_tfidf = clf.predict(X_test_tfidf)\n","print('TDIDF Accuracy:', f1_score(target_test, target_test_pred_tfidf))"],"execution_count":0,"outputs":[{"output_type":"stream","text":["TDIDF Accuracy: 0.7647336162187647\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"D2RCK3uD40fy","colab_type":"text"},"source":["# Creating Clusters...\n"]},{"cell_type":"code","metadata":{"id":"vU9-mTAVUjZR","colab_type":"code","outputId":"f194d9f3-aa76-4452-b1fc-ea0132ce043a","executionInfo":{"status":"ok","timestamp":1583472646874,"user_tz":300,"elapsed":1149,"user":{"displayName":"Devanshee Shah","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiPK61duZO3eD2_k6CihN89Q3LYOuSUxY6MRWaz4Q=s64","userId":"17675293810520504748"}},"colab":{"base_uri":"https://localhost:8080/","height":85}},"source":["import nltk.data\n","import pandas as pd\n","from bs4 import BeautifulSoup\n","import re\n","from nltk.corpus import stopwords\n","import numpy as np\n","from sklearn.ensemble import RandomForestClassifier\n","from sklearn.model_selection import train_test_split\n","from sklearn.model_selection import cross_val_score\n","from sklearn.metrics import f1_score\n","nltk.download('punkt')\n","nltk.download('stopwords')\n","tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')"],"execution_count":0,"outputs":[{"output_type":"stream","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt.zip.\n","[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"B7XIJF0Kn995","colab_type":"code","colab":{}},"source":["msk = np.random.rand(len(data)) < 0.8\n","train = data[msk]\n","test = data[~msk]\n","\n","#Clean the reviews and add them to the list below\n","reviews_train_clean = []\n","reviews_test_clean = []\n","#Write your code below.\n","num_reviews = train[\"text\"].size\n","tlist=train[\"text\"].tolist()\n","for i in range(num_reviews):\n","  cr=preprocess_review(tlist[i])\n","  reviews_train_clean.append(cr)\n","\n","num_reviews = test[\"text\"].size\n","tlist=test[\"text\"].tolist()\n","for i in range(num_reviews):\n","    cr=preprocess_review(tlist[i])\n","    reviews_test_clean.append(cr)    "],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"wXfJS3831VSr","colab_type":"text"},"source":["Creating reviews based on unique users"]},{"cell_type":"code","metadata":{"id":"DXtOY6PNoY_R","colab_type":"code","outputId":"bf8aec6c-4322-4779-9ed9-a77b74d29b69","executionInfo":{"status":"error","timestamp":1583481724444,"user_tz":300,"elapsed":1221,"user":{"displayName":"Devanshee Shah","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiPK61duZO3eD2_k6CihN89Q3LYOuSUxY6MRWaz4Q=s64","userId":"17675293810520504748"}},"colab":{"base_uri":"https://localhost:8080/","height":936}},"source":["unique_id=np.unique(train['user_id'])\n","print( train)\n","# print(unique_id)\n","# print(train['text'][0])\n","user_based_reviews=[]\n","counter=0\n","print(train.loc[10000])\n","# for uid in unique_id:\n","#   id=np.where(train['user_id']==uid)\n","#   print(counter, id)\n","#   rev=\"\"\n","#   for i in range(len(id[0])):\n","#     print(\"inside\",i)\n","#     print(train['text'][id[0][i]])\n","#     rev=rev+\" \"+(train['text'][id[0][i]]) \n","#   user_based_reviews.append([uid,rev])\n","#   counter+=1\n","  \n","# print(rev)\n","#print(train[np.where(train['user_id'] == 'uFVAAe0JC81IPmxgT49Hcw')])\n","# for uid in unique_id:\n","#   print(uid, train[uid])\n"],"execution_count":0,"outputs":[{"output_type":"stream","text":["                    review_id  ... Good Review\n","0      hUUfzKeDLo930LA9-v2tWQ  ...         1.0\n","1      c4t3x4G2goB2OuqrEiBzJA  ...         1.0\n","2      ZZXyJhzMoY1SFvyp5bRGgw  ...         0.0\n","3      MVB0A9akvrnz-ICyEfPFgQ  ...         0.0\n","4      SOSBVld0AsaO-uFULble1Q  ...         1.0\n","...                       ...  ...         ...\n","12656  s01nkwKtmAzZc9CbnZiqMQ  ...         0.0\n","12657  OU0-l3w4CcItPx35guOaJA  ...         1.0\n","12658  1FvT4lC86tahz4kQUpujGQ  ...         0.0\n","12660  I3LVHf4Y493-igtIzPOEjg  ...         0.0\n","12661  _85TjsM5SFTfGp7J66o1tw  ...         0.0\n","\n","[10079 rows x 12 columns]\n"],"name":"stdout"},{"output_type":"error","ename":"KeyError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   2896\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2897\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2898\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n","\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n","\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.Int64HashTable.get_item\u001b[0;34m()\u001b[0m\n","\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.Int64HashTable.get_item\u001b[0;34m()\u001b[0m\n","\u001b[0;31mKeyError\u001b[0m: 10000","\nDuring handling of the above exception, another exception occurred:\n","\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)","\u001b[0;32m<ipython-input-152-aca309ae7d9b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0muser_based_reviews\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mcounter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m10000\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;31m# for uid in unique_id:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;31m#   id=np.where(train['user_id']==uid)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   1422\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1423\u001b[0m             \u001b[0mmaybe_callable\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_if_callable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1424\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_axis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmaybe_callable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1425\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1426\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_is_scalar_access\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m_getitem_axis\u001b[0;34m(self, key, axis)\u001b[0m\n\u001b[1;32m   1848\u001b[0m         \u001b[0;31m# fall thru to straight lookup\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1849\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_key\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1850\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_label\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1851\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1852\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m_get_label\u001b[0;34m(self, label, axis)\u001b[0m\n\u001b[1;32m    158\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mIndexingError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"no slices here, handle elsewhere\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    159\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 160\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_xs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    161\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    162\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_get_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36mxs\u001b[0;34m(self, key, axis, level, drop_level)\u001b[0m\n\u001b[1;32m   3735\u001b[0m             \u001b[0mloc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc_level\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdrop_level\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdrop_level\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3736\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3737\u001b[0;31m             \u001b[0mloc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3738\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3739\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   2897\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2898\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2899\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_cast_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2900\u001b[0m         \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtolerance\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtolerance\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2901\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n","\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n","\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.Int64HashTable.get_item\u001b[0;34m()\u001b[0m\n","\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.Int64HashTable.get_item\u001b[0;34m()\u001b[0m\n","\u001b[0;31mKeyError\u001b[0m: 10000"]}]},{"cell_type":"markdown","metadata":{"id":"fj_LahFf1am2","colab_type":"text"},"source":["Creating reviews based on Restaurants "]},{"cell_type":"code","metadata":{"id":"z-UYYApV1gTi","colab_type":"code","outputId":"f83c8089-b6bd-4eaa-acf3-c58e1021833f","executionInfo":{"status":"error","timestamp":1583481400950,"user_tz":300,"elapsed":687,"user":{"displayName":"Devanshee Shah","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiPK61duZO3eD2_k6CihN89Q3LYOuSUxY6MRWaz4Q=s64","userId":"17675293810520504748"}},"colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["unique_id=np.unique(train['restaurant_name'])\n","print(unique_id.shape)\n","pri\n","print(train['restaurant_name'][0])\n","print( np.where(train['restaurant_name']=='Eest Asian Bistro'))\n","user_based_reviews=[]\n","counter=0\n","for uid in unique_id:\n","  id=np.where(train['restaurant_name']==uid)\n","  print(counter, id)\n","  rev=\"\"\n","  for i in range(len(id[0])):\n","    print(\"inside\",i)\n","    print(train['text'][id[0][i]])\n","    rev=rev+\" \"+(train['text'][id[0][i]]) \n","  user_based_reviews.append([uid,rev])\n","  counter+=1\n","  \n","# print(rev)\n","#print(train[np.where(train['user_id'] == 'uFVAAe0JC81IPmxgT49Hcw')])\n","# for uid in unique_id:\n","#   print(uid, train[uid])\n"],"execution_count":0,"outputs":[{"output_type":"stream","text":["(163,)\n","Eest Asian Bistro\n","(array([  0,   4,  14,  16,  19,  25,  29,  35,  36,  38,  46,  49,  51,\n","        57,  60,  61,  62,  80,  85,  88,  98, 111, 128, 131, 137, 138,\n","       150, 153, 156, 157, 181, 193, 195, 198, 204, 218, 222, 225, 228,\n","       233, 234, 238, 244, 250, 264, 267, 271, 273, 276, 279, 289, 301,\n","       311, 322, 324, 327, 340, 347, 354, 359, 370, 384, 404, 409, 424,\n","       425, 427, 429, 432, 446, 462, 466, 470, 471, 472, 476, 484, 486,\n","       491, 495, 504, 506, 509, 511, 516, 520, 529, 531, 541, 560, 564,\n","       572, 576, 579, 580, 581, 592, 606, 610, 618, 623, 631, 637, 640,\n","       644, 646, 654, 662, 671, 675, 676, 687, 691, 695, 699, 700, 710,\n","       721, 726, 736, 740, 741, 743, 756, 764]),)\n","0 (array([ 10,  23,  43,  79,  86,  91, 104, 113, 115, 172, 175, 179, 206,\n","       211, 268, 278, 350, 355, 357, 373, 416, 431, 459, 474, 482, 505,\n","       552, 553, 557, 578, 600, 608, 626, 630, 647, 656, 661, 664, 683,\n","       707, 734, 737, 751, 771]),)\n","inside 0\n","Worst experience I've ever had with a restaurant. Theresa was our  waitress and was nice at first, but all she did was take our order and drop off the check. No refills, no checking up, nothing. Then I woke up the next morning with food poisoning from my  chicken. Thanks IHOP.\n","inside 1\n","So far the only place that I've tried that was good. Been searching for a year since I moved for a good place. Overall happy. I got the garlic chicken and Mongolian sizzling chicken. Delicious. Plus it's quiet and the staff are great.\n","inside 2\n","The food was delicious but the service was pretty lacking. Took forever to get refills and had to ask a different server for items we asked our waitress for. The Italian lemon cake taste very similar to macaroni grills but isn't as moist.\n","inside 3\n","REVIEW IS FOR CLAIM JUMPER. \n","\n","Absolutely terrible service. We waited forever for our server to take our food and even longer to get our food. Our server took forever to get us coffee and drink refills. \n","\n","My mom's order came out incorrect with cold food and the chicken tasted so bad that she left it. She ordered her potato w extra butter and instead it came out cold with sour cream. There was no use asking for it to be fixed because every request took about 10 minutes to be fulfilled. \n","\n","I drove in from out of state to celebrate both of my parent's birthdays and they chose this place. I told the hostess, the server and whom I believe to be the manager back in the kitchen that it was their birthdays. Not one single person said happy birthday to them as most restaurants do. \n","\n","I ordered chocolate cake to go and the box was delivered with STICKY chocolate fingerprints ALL over the box and I had to wipe it down and ask for a bag. \n","\n","The server did seem he was irritated that we asked to move tables at the very beginning. We were all VERY friendly to the server but even so it made not an ounce of difference. I could post the server and manager's name but hopefully this review will suffice in them getting their act together. But I won't find out because I will never return. \n","\n","Unfortunately it could not have been more disappointing experience and I certainly will never return. I ADVISE everyone to steer clear of this establishment.\n","\n","Trust me. You will receive better service at McDonalds. \n","And management please feel free to reach out for specifics.\n","inside 4\n","Cool place! This used to be Johnny Rockets! Now it's called JR's Burgers. They have a very simple menu and the burgers here are delicious!\n","inside 5\n"],"name":"stdout"},{"output_type":"error","ename":"KeyError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)","\u001b[0;32m<ipython-input-139-d1d036ef2d9e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m   \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"inside\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'text'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m     \u001b[0mrev\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrev\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m\" \"\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'text'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m   \u001b[0muser_based_reviews\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0muid\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mrev\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/core/series.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   1069\u001b[0m         \u001b[0mkey\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_if_callable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1070\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1071\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1072\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1073\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_scalar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_value\u001b[0;34m(self, series, key)\u001b[0m\n\u001b[1;32m   4728\u001b[0m         \u001b[0mk\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_convert_scalar_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkind\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"getitem\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4729\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4730\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtz\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseries\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"tz\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4731\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4732\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mholds_integer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_boolean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_value\u001b[0;34m()\u001b[0m\n","\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_value\u001b[0;34m()\u001b[0m\n","\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n","\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.Int64HashTable.get_item\u001b[0;34m()\u001b[0m\n","\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.Int64HashTable.get_item\u001b[0;34m()\u001b[0m\n","\u001b[0;31mKeyError\u001b[0m: 91"]}]},{"cell_type":"code","metadata":{"id":"FnCwjTvRUZU_","colab_type":"code","colab":{}},"source":["def clean_review(review, remove_stopwords = False):\n","    \"\"\"Helper function to clean the reviews i.e. to convert a document to a sequence of words.\n","     Please note that we're not removing stopwords since word2vec relies on the broader context\n","     of the sentence in order to produce high-quality word vectors.\n","\n","     Arg: review: review string (str)\n","          remove_stopwards: If true remove stopwords else not. (boolean)\n","     Returns: cleaned_review : Cleaned review (list)\n","\n","     You should carry out the following steps.\n","     1. Remove HTML Tags.\n","     2. Remove non-letter characters.\n","     3. Convert to lower case.\n","    \"\"\"\n","    ### Add your code here.\n","    review_without_html = BeautifulSoup(review).get_text()\n","    review_only_lettes = re.sub(\"[^a-zA-Z]\",\" \", review_without_html)\n","    cleaned_review = review_only_lettes.lower().split()\n","    #####################\n","    if remove_stopwords:\n","        stops = set(stopwords.words(\"english\"))\n","        cleaned_review = [w for w in cleaned_review if not w in stops]\n","    return cleaned_review"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"oijHVma2Ufz9","colab_type":"code","colab":{}},"source":["def review_to_sentences( review: str, tokenizer: nltk.tokenize.punkt.PunktSentenceTokenizer ):\n","    \"\"\"Helper function to split a review into parsed sentences. Returns a \n","     list of sentences, where each sentence is a list of words.\n","\n","     Arg: review: review string (str)\n","          tokenizer: punkt tokenizer\n","     Returns:\n","          review_sentences: List of list of tokens.\n","                            e.g. [[\"word2vec\", \"was\", \"introduced\", \"by\", \"google\" ],[\"it\",\"leverages\",\"distributed\",\"token\",\"representations\"]]\n","\n","     You should carry out the following steps.\n","     1. Use the tokenizer to split the paragraph into sentences.\n","     2. Clean the sentence to return a list of words for each sentence using the helper funtion above.\n","     3. Return a list of tokenized sentences.\n","    \"\"\"\n","    ### Add your code here.\n","    tokenized_sentences = tokenizer.tokenize(review.strip())\n","    review_sentences = []\n","    for sentence in tokenized_sentences:\n","        if len(sentence) > 0:\n","            review_sentences.append( clean_review( sentence, remove_stopwords=True ))\n","    ######################\n","    \n","    return review_sentences"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"whCcR_9tUurC","colab_type":"code","outputId":"95187b39-ac30-4ec5-aa2e-43673ea02ef4","executionInfo":{"status":"ok","timestamp":1583474932411,"user_tz":300,"elapsed":6798,"user":{"displayName":"Devanshee Shah","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiPK61duZO3eD2_k6CihN89Q3LYOuSUxY6MRWaz4Q=s64","userId":"17675293810520504748"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["sentences = []  # Initialize an empty list of sentences\n","\n","print(\"Parsing sentences from training set\")\n","### Add your code here.\n","for review in reviews_train_clean:\n","    sentences += review_to_sentences(review, tokenizer)\n"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Parsing sentences from training set\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"JCT2Fkrdd1by","colab_type":"code","colab":{}},"source":["from gensim.models import word2vec, KeyedVectors"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"p5U3j4u_d-AJ","colab_type":"code","colab":{}},"source":["def generate_z1(sentences,num_features,min_word_count,context,num_workers = 4,downsampling = 1e-3, model_name = \"model1_100features_40minwords_5context\"):\n","    \"\"\"Set values for the parameters of the your word2vec model and train it on the extracted sentences from both the train and unlabeled_train data and\n","      generate the collection of these representations Z1.\n","      You should carry out the following.\n","      1) Set the parameters as mentioned below. \n","        A)Constrained Paramters : \"context length\", \"embedding dimension\", \"min_words\" (Please check the question for the values.)\n","        B)Optional Parameters: \"number of workers\", \"downsample setting\"\n","      2) Train your word2vec model and save it.\n","      3) Store the collection of word embeddings and the word_list(z1 and word_list_z1) .\n","\n","      Arg: sentences: List of tokenized sentences (List)\n","            num_features: Word vector dimensionality (int)\n","            min_word_count: Minimum word count (int)\n","            context: Context window size (int)\n","            num_workers: Number of threads to run in parallel (int)\n","            downsampling: Downsample setting for frequent words (float)\n","            model_name = Name to save your model (str)\n","      Returns:\n","            trained_word2vec_model: word2vec model trained on the tokenized sentences.\n","            z1: word embeddings (ndarray)\n","            word_list_z1: List of tokens in the model (List)\n","\n","      \n","\n","    \"\"\"\n","    print(\"Training model...\")\n","    ### Add your code here.\n","    trained_word2vec_model = word2vec.Word2Vec(sentences, workers=num_workers, \\\n","            size=num_features, min_count = min_word_count, \\\n","            window = context, sample = downsampling)\n","    trained_word2vec_model.save(model_name)\n","\n","    #######################\n","    # word_list_z1 = set(model.index2word)\n","    word_list_z1 = trained_word2vec_model.wv.index2word\n","    #print(len(word_list_z1))\n","    z1 = trained_word2vec_model.wv[word_list_z1]\n","    # print(\"shape\",z1.shape)\n","    # words = w2v_model.wv.index2word\n","    return trained_word2vec_model, z1, word_list_z1\n","    \n","# Set values for the parameters of the your word2vec model and train it on the extracted sentences from both the train and unlabeled_train data and\n","# generate the collection of these representations Z1.\n","# Add the function parameter values below.\n","### Add your code here.\n","num_features=100\n","min_word_count=40\n","context=5\n","\n","\n","\n","######################\n","# generate_z1(sentences,num_features,min_word_count,context)\n","\n","model1, z1, word_list_z1 = generate_z1(sentences,num_features,min_word_count,context)\n","print(len(word_list_z1))\n","print(\"shape\",z1.shape)\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"MM93ffpVet9B","colab_type":"code","colab":{}},"source":["from sklearn.cluster import KMeans\n","import time"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"l7fb9pM-epEQ","colab_type":"code","outputId":"4b587f17-7297-41ae-9414-9963ba70490c","executionInfo":{"status":"ok","timestamp":1583475398110,"user_tz":300,"elapsed":1730,"user":{"displayName":"Devanshee Shah","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiPK61duZO3eD2_k6CihN89Q3LYOuSUxY6MRWaz4Q=s64","userId":"17675293810520504748"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["def fit_kmeans(z,word_list_z,num_clusters = 10):\n","    \"\"\" Fit kmeans on the embedding representations and return a mapping of word to cluster indices. Please use the default values for\n","        the rest of the kmeans parameters.\n","\n","        Arg: z: word embeddings (ndarray)\n","              word_list_z: List of tokens in the model (List) \n","              num_clusters: Number of clusters (int)\n","        Returns:\n","            pre_trained_word2vec_model: word2vec model trained on the tokenized sentences.\n","            z2: word embeddings (ndarray)\n","            word_centroid_map_z: A mapping of word to cluster index it belongs to. (Dict)      \n","\n","    \"\"\"\n","    ### Add your code here.\n","    start = time.time() # Start time\n","\n","    kmeans = KMeans(n_clusters=num_clusters, random_state=0).fit_predict(z)\n","    #print(kmeans.labels_.shape)\n","    word_centroid_map_z = dict(zip( word_list_z, kmeans ))\n","    ######################\n","    end = time.time()\n","    elapsed = end - start\n","    print (\"Time taken for K Means clustering: \", elapsed, \"seconds.\")\n","    return word_centroid_map_z\n","\n","#fit_kmeans(z1, word_list_z1, 10)\n","\n","word_centroid_map_z1 = fit_kmeans(z1, word_list_z1, 10)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Time taken for K Means clustering:  0.7413699626922607 seconds.\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"FbQaKaMcfiIj","colab_type":"code","outputId":"4c5f6f9a-851a-4990-cac7-cb398faaf5ee","executionInfo":{"status":"ok","timestamp":1583475481380,"user_tz":300,"elapsed":1194,"user":{"displayName":"Devanshee Shah","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiPK61duZO3eD2_k6CihN89Q3LYOuSUxY6MRWaz4Q=s64","userId":"17675293810520504748"}},"colab":{"base_uri":"https://localhost:8080/","height":224}},"source":["def print_clusters(word_centroid_map_z, model_name):\n","    \"\"\" Print max(20, cluster_size) words for each of the clusters.\n","\n","        Args: word_centroid_map_z: A mapping of word to cluster index it belongs to. (Dict)  \n","              model_name: Model Name (str)\n","\n","\n","    \"\"\"\n","    print(\"The clusters for {0} are....\".format(model_name))\n","    ### Add your code here.\n","    n=20\n","    num_clusters = max( word_centroid_map_z.values() ) + 1\n","    for cluster in range(num_clusters):\n","      words = []\n","      for i in range(len(word_centroid_map_z.values())):\n","        if (len(words)<=n):\n","          if( list(word_centroid_map_z.values())[i] == cluster ):\n","              words.append(list(word_centroid_map_z.keys())[i])\n","        else:\n","          break\n","      print (cluster,\" : \",words)\n","\n","    ######################\n","print_clusters(word_centroid_map_z1, \"model1\")"],"execution_count":0,"outputs":[{"output_type":"stream","text":["The clusters for model1 are....\n","0  :  ['place', 'best', 'mexican', 'ever', 'new', 'favorite', 'area', 'many', 'find', 'places', 'thai', 'found', 'looking', 'valley', 'west', 'authentic', 'far', 'spot', 'restaurants', 'avondale', 'town']\n","1  :  ['like', 'pizza', 'little', 'menu', 'much', 'made', 'meal', 'small', 'bit', 'thing', 'ok', 'big', 'different', 'nothing', 'enough', 'large', 'full', 'used', 'usually', 'inside', 'thought']\n","2  :  ['chicken', 'ordered', 'sauce', 'fries', 'rice', 'salad', 'cheese', 'fried', 'meat', 'flavor', 'shrimp', 'bread', 'soup', 'tasted', 'tacos', 'sandwich', 'beans', 'beef', 'spicy', 'cooked', 'red']\n","3  :  ['get', 'one', 'would', 'even', 'never', 'could', 'make', 'know', 'give', 'sure', 'want', 'right', 'two', 'still', 'home', 'something', 'see', 'though', 'wanted', 'free', 'eating']\n","4  :  ['got', 'also', 'burger', 'hot', 'tried', 'taste', 'side', 'husband', 'salsa', 'chips', 'sweet', 'tea', 'wife', 'huge', 'served', 'perfect', 'cold', 'ice', 'wings', 'top', 'extra']\n","5  :  ['order', 'us', 'came', 'said', 'wait', 'minutes', 'asked', 'took', 'take', 'table', 'manager', 'drinks', 'told', 'server', 'long', 'another', 'drink', 'waitress', 'ask', 'half', 'away']\n","6  :  ['good', 'great', 'really', 'love', 'delicious', 'well', 'fresh', 'definitely', 'amazing', 'everything', 'pretty', 'recommend', 'excellent', 'awesome', 'tasty', 'price', 'prices', 'quality', 'loved', 'overall', 'greek']\n","7  :  ['food', 'service', 'always', 'restaurant', 'friendly', 'nice', 'staff', 'clean', 'customer', 'people', 'experience', 'fast', 'super', 'busy', 'bar', 'atmosphere', 'quick', 'feel', 'employees', 'everyone', 'owner']\n","8  :  ['time', 'first', 'went', 'location', 'lunch', 'times', 'family', 'last', 'since', 'every', 'next', 'dinner', 'day', 'visit', 'around', 'night', 'drive', 'today', 'decided', 'years', 'breakfast']\n","9  :  ['back', 'go', 'try', 'come', 'eat', 'better', 'going', 'way', 'bad', 'say', 'think', 'coming', 'stars', 'worth', 'lot', 'disappointed', 'however', 'happy', 'need', 'probably', 'enjoy']\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"hcErAkifgzPK","colab_type":"code","outputId":"59240180-9181-4950-cab6-5dbe500ab1f2","executionInfo":{"status":"ok","timestamp":1583476135903,"user_tz":300,"elapsed":1105,"user":{"displayName":"Devanshee Shah","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiPK61duZO3eD2_k6CihN89Q3LYOuSUxY6MRWaz4Q=s64","userId":"17675293810520504748"}},"colab":{"base_uri":"https://localhost:8080/","height":360}},"source":["print(word_list_z1)\n","print(word_centroid_map_z1['good'])\n","print(model1[\"thai\"])"],"execution_count":0,"outputs":[{"output_type":"stream","text":["['food', 'good', 'place', 'great', 'service', 'like', 'time', 'back', 'get', 'one', 'go', 'order', 'chicken', 'would', 'really', 'ordered', 'got', 'always', 'love', 'restaurant', 'delicious', 'us', 'try', 'friendly', 'also', 'nice', 'best', 'even', 'first', 'came', 'staff', 'pizza', 'never', 'well', 'little', 'fresh', 'sauce', 'come', 'went', 'definitely', 'fries', 'eat', 'could', 'rice', 'amazing', 'menu', 'better', 'salad', 'much', 'location', 'going', 'made', 'burger', 'said', 'lunch', 'wait', 'clean', 'minutes', 'customer', 'people', 'everything', 'make', 'know', 'meal', 'cheese', 'pretty', 'give', 'asked', 'times', 'mexican', 'ever', 'sure', 'way', 'bad', 'say', 'want', 'hot', 'new', 'family', 'tried', 'right', 'experience', 'two', 'fried', 'took', 'still', 'take', 'meat', 'taste', 'flavor', 'recommend', 'side', 'last', 'table', 'since', 'shrimp', 'favorite', 'manager', 'home', 'excellent', 'fast', 'every', 'drinks', 'awesome', 'next', 'think', 'told', 'server', 'husband', 'small', 'something', 'long', 'dinner', 'area', 'salsa', 'coming', 'bit', 'day', 'chips', 'bread', 'another', 'super', 'drink', 'busy', 'visit', 'tasty', 'stars', 'around', 'thing', 'soup', 'see', 'though', 'bar', 'price', 'night', 'waitress', 'prices', 'tasted', 'worth', 'ok', 'many', 'sweet', 'wanted', 'lot', 'tacos', 'big', 'sandwich', 'different', 'beans', 'quality', 'find', 'nothing', 'beef', 'places', 'thai', 'disappointed', 'tea', 'drive', 'however', 'today', 'enough', 'ask', 'wife', 'happy', 'spicy', 'decided', 'found', 'free', 'looking', 'half', 'cooked', 'eating', 'atmosphere', 'loved', 'away', 'red', 'plate', 'left', 'pita', 'put', 'valley', 'overall', 'large', 'gave', 'cream', 'west', 'maybe', 'quick', 'huge', 'served', 'years', 'waiting', 'finally', 'greek', 'else', 'let', 'perfect', 'cold', 'authentic', 'enjoyed', 'ice', 'wrong', 'full', 'need', 'tables', 'burrito', 'used', 'customers', 'feel', 'brought', 'usually', 'check', 'portions', 'wings', 'burgers', 'kids', 'waited', 'fish', 'top', 'anything', 'extra', 'looked', 'far', 'probably', 'things', 'inside', 'breakfast', 'work', 'thought', 'spot', 'kind', 'hour', 'steak', 'absolutely', 'enjoy', 'special', 'taco', 'look', 'restaurants', 'business', 'walked', 'bowl', 'review', 'reviews', 'almost', 'coffee', 'actually', 'tell', 'employees', 'old', 'green', 'everyone', 'buffet', 'couple', 'owner', 'hard', 'getting', 'avondale', 'dish', 'star', 'money', 'must', 'items', 'dishes', 'boba', 'gyro', 'trying', 'least', 'stop', 'line', 'ate', 'orders', 'flavors', 'several', 'use', 'keep', 'oh', 'comes', 'wonderful', 'okay', 'yummy', 'decent', 'quite', 'without', 'counter', 'done', 'sushi', 'kitchen', 'front', 'horrible', 'either', 'return', 'slow', 'three', 'whole', 'house', 'seemed', 'close', 'town', 'attentive', 'friends', 'instead', 'options', 'garlic', 'open', 'point', 'highly', 'worst', 'water', 'rude', 'fantastic', 'care', 'called', 'especially', 'crepes', 'dry', 'flavorful', 'yelp', 'roll', 'extremely', 'phoenix', 'stopped', 'second', 'wish', 'sat', 'someone', 'hummus', 'size', 'style', 'curry', 'friend', 'real', 'toppings', 'dirty', 'dining', 'girl', 'later', 'person', 'helpful', 'gumbo', 'week', 'po', 'catfish', 'waiter', 'fan', 'bbq', 'asada', 'combo', 'makes', 'high', 'pay', 'ready', 'soon', 'liked', 'amount', 'call', 'bring', 'boy', 'cajun', 'bland', 'ordering', 'guy', 'less', 'pm', 'crepe', 'selection', 'bacon', 'part', 'pick', 'n', 'impressed', 'name', 'regular', 'meals', 'lady', 'leave', 'rolls', 'add', 'reason', 'making', 'daughter', 'carne', 'pork', 'chinese', 'onion', 'arizona', 'plus', 'reasonable', 'crust', 'portion', 'crispy', 'warm', 'guess', 'seated', 'glad', 'eaten', 'bite', 'serve', 'working', 'appetizer', 'lettuce', 'average', 'party', 'saw', 'outside', 'dessert', 'arrived', 'quickly', 'beer', 'terrible', 'ago', 'louisiana', 'door', 'store', 'cashier', 'priced', 'often', 'felt', 'year', 'thru', 'already', 'gone', 'taking', 'seem', 'thank', 'seems', 'yet', 'grilled', 'chili', 'variety', 'lots', 'offer', 'deal', 'start', 'egg', 'fine', 'may', 'started', 'anyone', 'yes', 'might', 'received', 'fun', 'although', 'tonight', 'expect', 'pie', 'paid', 'walk', 'bill', 'phone', 'chipotle', 'help', 'son', 'job', 'wow', 'perfectly', 'fact', 'sit', 'behind', 'live', 'music', 'sitting', 'twice', 'cup', 'indian', 'opened', 'yum', 'gets', 'white', 'empty', 'able', 'man', 'room', 'tomatoes', 'greasy', 'problem', 'sandwiches', 'choice', 'oil', 'offered', 'french', 'literally', 'pleasant', 'change', 'needs', 'owners', 'cut', 'needed', 'ingredients', 'sour', 'hungry', 'believe', 'chain', 'past', 'choose', 'surprised', 'please', 'cook', 'management', 'grill', 'boyfriend', 'veggies', 'seafood', 'onions', 'rather', 'street', 'looks', 'run', 'hit', 'kept', 'returning', 'hope', 'mind', 'young', 'tender', 'saturday', 'simple', 'seasoned', 'five', 'forgot', 'friday', 'mom', 'decor', 'packed', 'tastes', 'guys', 'end', 'ended', 'rest', 'choices', 'attitude', 'potato', 'given', 'sausage', 'saying', 'local', 'crab', 'servers', 'hours', 'az', 'soft', 'plenty', 'craving', 'card', 'giving', 'hands', 'min', 'days', 'sometimes', 'course', 'short', 'closed', 'mins', 'totally', 'greeted', 'stuff', 'cheap', 'across', 'others', 'near', 'heard', 'thin', 'employee', 'birthday', 'understand', 'morning', 'noticed', 'added', 'sorry', 'toast', 'potatoes', 'chile', 'plates', 'poor', 'unfortunately', 'longer', 'tasting', 'four', 'cool', 'establishment', 'easy', 'excited', 'fry', 'expensive', 'miss', 'enchiladas', 'parking', 'minute', 'outstanding', 'salads', 'sign', 'box', 'game', 'remember', 'dressing', 'pricey', 'late', 'taken', 'seriously', 'chocolate', 'homemade', 'piece', 'generous', 'enchilada', 'mango', 'charge', 'type', 'complaint', 'frozen', 'sunday', 'thanks', 'light', 'entire', 'finish', 'joint', 'avocado', 'gyros', 'coupon', 'owned', 'pieces', 'double', 'smile', 'talk', 'prepared', 'expected', 'register', 'mean', 'tip', 'la', 'mouth', 'life', 'bun', 'tomato', 'games', 'knew', 'lol', 'salty', 'olive', 'rings', 'option', 'mixed', 'available', 'bowling', 'cooking', 'issue', 'strawberry', 'tortillas', 'disappointing', 'hubby', 'recently', 'completely', 'crawfish', 'due', 'gross', 'seating', 'workers', 'soda', 'eggs', 'main', 'missing', 'happened', 'seen', 'california', 'visited', 'lamb', 'immediately', 'recommended', 'online', 'surprise', 'teriyaki', 'honestly', 'attention', 'awful', 'delivery', 'opinion', 'says', 'placed', 'cost', 'forward', 'talking', 'filled', 'specials', 'waste', 'shredded', 'milk', 'chance', 'based', 'mine', 'pasta', 'number', 'etc', 'moved', 'corn', 'read', 'together', 'finished', 'asian', 'de', 'sad', 'pizzas', 'th', 'bag', 'pickles', 'group', 'mediterranean', 'sauces', 'window', 'share', 'black', 'loves', 'burritos', 'bomb', 'environment', 'entree', 'asking', 'trip', 'goes', 'weekend', 'ones', 'charged', 'serving', 'polite', 'iced', 'hand', 'torta', 'pad', 'play', 'bell', 'brown', 'healthy', 'item', 'welcoming', 'takes', 'heat', 'noodles', 'soggy', 'medium', 'mention', 'hostess', 'thick', 'shawarma', 'filling', 'matter', 'exactly', 'along', 'tortilla', 'thinking', 'garden', 'cafe', 'lemon', 'grab', 'satisfied', 'wine', 'smash', 'write', 'salt', 'unless', 'ribs', 'shop', 'visiting', 'juicy', 'usual', 'simply', 'crisp', 'w', 'treat', 'normally', 'vegetarian', 'cute', 'delivered', 'months', 'pepper', 'fair', 'turkey', 'leaving', 'kinda', 'wall', 'idea', 'plain', 'burnt', 'coconut', 'die', 'buy', 'mediocre', 'bartender', 'quesadilla', 'nutella', 'value', 'forever', 'spice', 'guacamole', 'veggie', 'anyway', 'disgusting', 'gluten', 'correct', 'sides', 'peach', 'changed', 'nasty', 'including', 'floor', 'tuesday', 'honey', 'cuisine', 'vegetables', 'fruit', 'running', 'playing', 'spend', 'checked', 'stay', 'kid', 'truly', 'upon', 'strips', 'month', 'expecting', 'hawaiian', 'gravy', 'total', 'picked', 'dont', 'mix', 'worse', 'locations', 'except', 'appetizers', 'nachos', 'loud', 'butter', 'supposed', 'entrees', 'boys', 'strip', 'mistake', 'bean', 'comfortable', 'e', 'paying', 'early', 'speak', 'somewhere', 'fairly', 'gem', 'deep', 'bottom', 'refills', 'mess', 'walking', 'crunchy', 'clearly', 'favorites', 'forget', 'mac', 'spring', 'middle', 'gotten', 'cake', 'within', 'fix', 'evening', 'watch', 'orange', 'consistent', 'single', 'hate', 'lost', 'girls', 'mood', 'slices', 'whatever', 'save', 'receipt', 'overpriced', 'pei', 'wei', 'naan', 'feeling', 'italian', 'low', 'margaritas', 'salmon', 'certainly', 'none', 'typical', 'woman', 'accommodating', 'prefer', 'ranch', 'show', 'set', 'spent', 'ambiance', 'glass', 'beyond', 'rating', 'goodyear', 'lack', 'covered', 'shot', 'afternoon', 'crowded', 'perfection', 'discount', 'hair', 'picky', 'dine', 'pop', 'located', 'complaints', 'sugar', 'car', 'claim', 'yesterday', 'crazy', 'island', 'dough', 'mall', 'cilantro', 'anymore', 'case', 'bought', 'word', 'desserts', 'weeks', 'dollars', 'general', 'weird', 'desert', 'compared', 'refund', 'delish', 'frybread', 'worked', 'affordable', 'driving', 'st', 'jambalaya', 'seasoning', 'sick', 'original', 'mild', 'u', 'alligator', 'cause', 'similar', 'traditional', 'refill', 'stuffed', 'ridiculous', 'world', 'barely', 'turned', 'professional', 'road', 'peppers', 'bowls', 'pass', 'omg', 'easily', 'seat', 'apologized', 'issues', 'positive', 'anywhere', 'dip', 'stick', 'list', 'multiple', 'standing', 'feta', 'rush', 'included', 'negative', 'disappointment', 'date', 'hear', 'bakery', 'yeah', 'soups', 'face', 'margarita', 'slightly', 'zeta', 'chorizo', 'cinnamon', 'pancakes', 'smaller', 'reasonably', 'touch', 'pulled', 'pleased', 'meats', 'basically', 'obviously', 'mushrooms', 'mentioned', 'otherwise', 'tough', 'machine', 'fat', 'stated', 'helped', 'incredible', 'mustard', 'hoping', 'slice', 'zero', 'showed', 'sub', 'future', 'overcooked', 'neighborhood', 'creamy', 'anyways', 'bigger', 'stand', 'note', 'lived', 'treated', 'chose', 'school', 'situation', 'sister', 'update', 'normal', 'avoid', 'non', 'yogurt', 'opening', 'falafel', 'shared', 'moist', 'bites', 'standard', 'platter', 'basic', 'continue', 'split', 'reading', 'complain', 'flat', 'smell', 'per', 'ahead', 'true', 'islands', 'banana', 'creole', 'panera', 'chewy', 'tiny', 'plan', 'im', 'monday', 'mayo', 'pepperoni', 'chef', 'personally', 'casual', 'con', 'kick', 'ran', 'seeing', 'texture', 'nd', 'gonna', 'head', 'robin', 'particular', 'foods', 'smashburger', 'mostly', 'cheeseburger', 'figured', 'fabulous', 'questions', 'leftovers', 'upset', 'third', 'consistently', 'american', 'disappoint', 'country', 'tortas', 'baked', 'topped', 'lacking', 'unique', 'par', 'lemonade', 'cooks', 'hold', 'east', 'children', 'tasteless', 'stood', 'explained', 'using', 'sucks', 'appreciate', 'subway', 'quiet', 'decide', 'notice', 'ketchup', 'express', 'sort', 'patty', 'closer', 'limited', 'cleaning', 'raw', 'tag', 'southern', 'kabob', 'training', 'nicely', 'guests', 'fill', 'topping', 'suggest', 'damn', 'fajitas', 'raspados', 'drove', 'macaroni', 'turn', 'savory', 'refreshing', 'hidden', 'break', 'pleasantly', 'compare', 'checking', 'ten', 'cups', 'sadly', 'c', 'level', 'become', 'el', 'grease', 'bucks', 'notch', 'awhile', 'booth', 'watched', 'messed', 'knows', 'prompt', 'bringing', 'biscuits', 'personal', 'figure', 'wanting', 'mongolian', 'snow', 'arrive', 'company', 'experienced', 'overly', 'beat', 'hamburger', 'threw', 'fancy', 'app', 'event', 'mashed', 'honest', 'native', 'visits', 'happen', 'b', 'higher', 'works', 'cannot', 'gourmet', 'ham', 'masala', 'orleans', 'broccoli', 'girlfriend', 'nearly', 'move', 'beautiful', 'patrons', 'considering', 'spoke', 'willing', 'sample', 'spinach', 'mother', 'throw', 'credit', 'india', 'bathroom', 'worker', 'vegan', 'eye', 'watching', 'courteous', 'frequent', 'dipping', 'shopping', 'cleanliness', 'city', 'realized', 'batter', 'carnitas', 'nypd', 'paradise', 'hopefully', 'cookies', 'ihop', 'solid', 'interesting', 'consider', 'loaded', 'freshly', 'vegetable', 'co', 'corner', 'selections', 'concept', 'stale', 'admit', 'laser', 'beignets', 'panda', 'mcdonald', 'horchata', 'miles', 'pot', 'stomach', 'patient', 'south', 'apart', 'p', 'alone', 'coke', 'landry', 'likely', 'menus', 'chairs', 'ladies', 'rolled', 'combination', 'whenever', 'tv', 'self', 'pineapple', 'raul', 'shake', 'ruby', 'napkins', 'expectations', 'fiesta', 'sizes', 'hush', 'exceptional', 'pho', 'classic', 'dog', 'trash', 'club', 'eyes', 'oven', 'ceviche', 'spices', 'groupon', 'cane', 'question', 'cash', 'baklava', 'mini', 'didnt', 'breading', 'efficient', 'apparently', 'sticky', 'realize', 'somewhat', 'smelled', 'melted', 'strong', 'means', 'sent', 'tons', 'earlier', 'jalape', 'space', 'cheesecake', 'watery', 'folks', 'plastic', 'culver', 'known', 'building', 'tuna', 'teas', 'pudding', 'puppies', 'hardly', 'unlimited', 'pricing', 'tells', 'beers', 'request', 'promptly', 'build', 'flavored', 'custard', 'everywhere', 'hurry', 'previous', 'welcome', 'possible', 'omelette', 'presentation', 'paper', 'tj', 'support', 'hole', 'finding', 'menudo', 'joke', 'rare', 'wide', 'explain', 'goodness', 'dollar', 'addition', 'needless', 'funny', 'closing', 'cards', 'undercooked', 'whether', 'provided', 'fell', 'def', 'zucchini', 'gives', 'hash', 'complimentary', 'booths', 'inviting', 'gentleman', 'consistency', 'clear', 'chicago', 'baby', 'movie', 'writing', 'larger', 'chunks', 'perhaps', 'moment', 'parents', 'mozzarella', 'product', 'picture', 'al', 'agree', 'telling', 'offers', 'receive', 'managers', 'favor', 'peanut', 'jumper', 'spots', 'rd', 'sized', 'helping', 'handle', 'fault', 'jack', 'book', 'straight', 'fired', 'bother', 'pico', 'mcdowell', 'takeout', 'military', 'fingers', 'coupons', 'sliced', 'fountain', 'bathrooms', 'likes', 'vibe', 'cookie', 'pastor', 'pieology', 'texas', 'aside', 'informed', 'pollo', 'difference', 'wonder', 'fire', 'lovely', 'crave', 'incredibly', 'doubt', 'shame', 'vanilla', 'container', 'flour', 'theresa', 'hey', 'spectacular', 'difficult', 'queso', 'fil', 'lane', 'rio', 'knowledgeable', 'nobody', 'reminded', 'step', 'mexicana', 'specifically', 'slaw', 'sell', 'dark', 'chimichanga', 'chick', 'habit', 'mein', 'cabbage', 'answered', 'nearby', 'include', 'filthy', 'gift', 'elsewhere', 'experiences', 'thumbs', 'meet', 'waitresses', 'till', 'restroom', 'prior', 'deliver', 'complete', 'types', 'pan', 'pictures', 'yellow', 'ono', 'cobbler', 'trust', 'moving', 'roasted', 'additional', 'enjoying', 'passed', 'noodle', 'met', 'sports', 'biggest', 'lime', 'alcohol', 'provide', 'response', 'recommendations', 'odd', 'salsas', 'lacked', 'repeat', 'nights', 'dried', 'confused', 'poke', 'wheat', 'tom', 'tired', 'crew', 'returned', 'heavy', 'excuse', 'happens', 'absolute', 'rate', 'everytime', 'enjoyable', 'conversation', 'actual', 'cleaned', 'older', 'decorated', 'alright', 'father', 'patio', 'balls', 'lentil', 'latte', 'opted', 'manner', 'feels', 'annoying', 'member', 'lower', 'neither', 'oz', 'hell', 'diet', 'separate', 'caramel', 'section', 'wing', 'pool', 'wet', 'phenomenal', 'buffets', 'skip', 'gotta', 'buffalo', 'ground', 'joints', 'fixed', 'sooo', 'sampler', 'state', 'handed', 'bunch', 'remake', 'team', 'drop', 'version', 'catch', 'lucky', 'bottle', 'strawberries', 'hang', 'basket', 'dysart', 'rich', 'despite', 'trio', 'station']\n","6\n","[ 1.054492   -0.22006604 -0.18204884 -0.15447845  0.17580284  0.6409949\n","  0.12984197  0.81362283 -0.58508056 -0.05748047 -0.67444843 -0.6253452\n","  0.8056529  -0.29725444 -0.31162494 -0.14000759  0.1176674   1.4163657\n"," -0.609247   -1.0557529  -0.06308663 -0.01968799  0.38583815 -0.10637417\n"," -0.15924262 -0.14077818  0.12111396 -0.7826956   0.3305613   1.2149757\n","  0.2548412   0.37363207 -0.41600603 -0.8448291  -0.08806621 -0.58623844\n","  0.226051    0.4692909  -0.755      -0.1802668  -0.12565754 -0.11548163\n","  0.38996118  0.5163539  -0.63233256 -0.03580274  0.13724945  0.5780416\n","  0.0099882   0.5470134  -0.17286174  0.15232871  0.34678525  0.44815066\n"," -0.536085    0.05849183  0.02605815 -0.63124406  0.05509976  0.48873785\n","  0.3423566  -0.5490572   0.17946663  0.3036074  -0.16623212  0.52060014\n"," -0.03658095 -0.21821208 -0.31956986 -0.6339159  -0.48547655 -0.45914555\n"," -0.786937   -0.972353   -0.30065024 -0.6351207  -0.36519644  0.36830232\n"," -0.3873566   0.35637563 -0.49105033 -0.23107187 -0.08886244 -0.9356226\n"," -0.10576052 -0.6533774  -0.09758928 -0.14049442  0.5879544  -0.65743357\n"," -0.71350044  0.07674667  0.55885947  1.0399655   0.8635149   0.6394774\n"," -0.15823555 -1.0424722  -0.30703172 -0.02010408]\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"vQSMIpLSkcJN","colab_type":"code","colab":{}},"source":["clean_train_reviews = []\n","clean_test_reviews = []\n","### Add your code here.\n","def preprocess_review(review):\n","    \"\"\"Helper function to clean the reviews.\n","\n","     Arg: review: review text.\n","     Returns: clean_review : Cleaned reviews\n","\n","     You should carry out the following steps.\n","     1. Remove HTML Tags.\n","     2. Remove non-letter characters.\n","     3. Convert to lower case.\n","     4. Remove stopwords.\n","    \"\"\"\n","\n","    #Write your code below.\n","    review_text = BeautifulSoup(review).get_text()\n","    letters_only = re.sub(\"[^a-zA-Z]\", \" \", review_text) \n","    words = letters_only.lower().split()\n","    stops = set(stopwords.words(\"english\")) \n","    meaningful_words = [w for w in words if not w in stops]\n","    clean_review= \" \".join( meaningful_words )\n","    \n","    return clean_review\n","\n","num_reviews = train[\"review\"].size\n","for i in range( 0, num_reviews ):\n","    # Call our function for each one, and add the result to the list of\n","    # clean reviews\n","    clean_train_reviews.append( preprocess_review( train[\"review\"][i] ) )\n","\n","num_rev = len(test[\"review\"])\n","\n","# Append the reviews one by one after preprocessing\n","for i in range(0,num_rev):\n","    clean_review = preprocess_review( test[\"review\"][i] )\n","    clean_test_reviews.append( clean_review )\n","\n","#######################"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"c9vDgoeEiZiB","colab_type":"code","colab":{}},"source":["def create_bag_of_centroids(tokenized_review, word_centroid_map, num_clusters = 10):\n","    \"\"\" Create a bag of kmeans centroids for each review i.e. for a review we return an array of length num_clusters with each \n","        element of the array indicating how many words(tokens) of the review belong to that cluster.\n","\n","        Args: tokenized_review: list of tokens corresponding to a review (List)\n","              word_centroid_map: word to cluster_index map for the model (Dict)\n","              num_clusters: Number of clusters (int)\n","        \n","        Returns: bag_of_centroids: An array containing the count of tokens in each cluster (ndarray)\n","    \"\"\"\n","  \n","    ### Add your code here.\n","    #\n","    # The number of clusters is equal to the highest cluster index\n","    # in the word / centroid map\n","    # num_centroids = max( word_centroid_map.values() ) + 1\n","    #\n","    # Pre-allocate the bag of centroids vector (for speed)\n","    bag_of_centroids = np.zeros( num_clusters, dtype=\"float32\" )\n","    #\n","    # Loop over the words in the review. If the word is in the vocabulary,\n","    # find which cluster it belongs to, and increment that cluster count \n","    # by one\n","    for word in tokenized_review:\n","        if word in word_centroid_map:\n","            index = word_centroid_map[word]\n","            bag_of_centroids[index] += 1    \n","    #######################\n","    return bag_of_centroids"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"zzZtTSRYjYvz","colab_type":"code","colab":{}},"source":["def create_design_matrices(data, cleaned_reviews, word_centroid_map_z1, num_clusters = 10):\n","    \"\"\" Creates the design matrices X1(for trained model) and X2(for pretrained google word2vec model) for the given data\n","\n","        Args: data: Train/Test data (pandas.core.frame.DataFrame)\n","              cleaned_reviews: List of tokenized reviews(sentences are not split and stopwords removed) (List)\n","              word_centroid_map_z1: word to cluster map for model1 (Dict)\n","              word_centroid_map_z2: word to cluster map for model2 (Dict)\n","              num_clusters: Number of KMeans clusters (int)\n","\n","        Returns:\n","              x1_data: Design matrices X1-- Shape should be num_reviews*num_clusters (np.ndarray) \n","              x2_data: Design matrices X2-- Shape should be num_reviews*num_clusters (np.ndarray)\n","\n","\n","    \"\"\"\n","    ### Add your code here.\n","    x1_data = np.zeros( (data[\"text\"].size, num_clusters),dtype=\"float32\" )\n","    counter = 0\n","    for review in cleaned_reviews:\n","      x1_data[counter] = create_bag_of_centroids( review, word_centroid_map_z1 )\n","      counter += 1\n","\n","    ######################\n","    return x1_data\n","x1_train = create_design_matrices(train,reviews_train_clean,word_centroid_map_z1,10)\n","x1_test = create_design_matrices(test,reviews_test_clean,word_centroid_map_z1,10)\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"vo78sl2klSuA","colab_type":"code","outputId":"e30867aa-1540-4c9a-eebe-8ccdeb4ae856","executionInfo":{"status":"ok","timestamp":1583477049449,"user_tz":300,"elapsed":1146,"user":{"displayName":"Devanshee Shah","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiPK61duZO3eD2_k6CihN89Q3LYOuSUxY6MRWaz4Q=s64","userId":"17675293810520504748"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["x1_train.shape\n"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(10161, 10)"]},"metadata":{"tags":[]},"execution_count":73}]}]}